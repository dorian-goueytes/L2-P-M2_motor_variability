{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPUFktMYGNcTLRAHnPCO8UQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dorian-goueytes/L2-P-M2_motor_variability/blob/main/TD_apprentissage_par_renforcement_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pré-requis"
      ],
      "metadata": {
        "id": "MxgJYcExJzI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation et importation des bibliothèques Python"
      ],
      "metadata": {
        "id": "A28E9gQvJ2wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ale-py\n",
        "!pip install gymnasium\n",
        "!pip install renderlab\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional\n",
        "import pickle\n",
        "import matplotlib.colors as mcolors\n",
        "import renderlab as rl\n",
        "from matplotlib.gridspec import GridSpec"
      ],
      "metadata": {
        "id": "8KZa2xLWXMAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fonctions nécessaires au TD"
      ],
      "metadata": {
        "id": "QX_3JZ6-J8sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_env(\n",
        "    env_name: str,\n",
        "    render_mode=\"rgb_array\",\n",
        ") -> gym.Env:\n",
        "    env = gym.make(env_name, render_mode=render_mode)\n",
        "\n",
        "    return env\n",
        "\n",
        "def plot_subdivided_heatmap(data,title = 'default title'):\n",
        "    #if data.shape != (16, 4):\n",
        "        #raise ValueError(\"Input array must have shape (16,4)\")\n",
        "\n",
        "    # Reshape the data into (4, 4, 2, 2)\n",
        "    if data.shape == (16, 4):\n",
        "      reshaped_data = data.reshape(4, 4, 2, 2)\n",
        "      # Create a new heatmap array of shape (8,8)\n",
        "      heatmap_data = np.block([[reshaped_data[i, j] for j in range(4)] for i in range(4)])\n",
        "      index = 8\n",
        "    if data.shape == (36, 4):\n",
        "      reshaped_data = data.reshape(6, 6, 2, 2)\n",
        "      # Create a new heatmap array of shape (8,8)\n",
        "      heatmap_data = np.block([[reshaped_data[i, j] for j in range(6)] for i in range(6)])\n",
        "      index = 12\n",
        "\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    cmap = plt.get_cmap(\"coolwarm\")\n",
        "    norm = mcolors.Normalize(vmin=np.min(0), vmax=np.max(1))\n",
        "\n",
        "    im = ax.imshow(heatmap_data, cmap=cmap, norm=norm)\n",
        "\n",
        "    # Add labels indicating identity (Action 1, Action 2, etc.)\n",
        "    action_labels = [[\"Left\", \"Down\"], [\"Right\", \"Up\"]]\n",
        "    for i in range(index):\n",
        "        for j in range(index):\n",
        "          label = action_labels[i % 2][j % 2]  # Assign Action labels based on position\n",
        "          ax.text(j, i, label, ha='center', va='center', color='black', fontsize=8)\n",
        "\n",
        "    # Add grid lines for subdivisions\n",
        "    ax.set_xticks(np.arange(-0.5, index, 2), minor=True)\n",
        "    ax.set_yticks(np.arange(-0.5, index, 2), minor=True)\n",
        "    ax.grid(which=\"minor\", color=\"black\", linestyle=\"-\", linewidth=2)\n",
        "    ax.tick_params(which=\"minor\", size=0)\n",
        "\n",
        "    # Remove major ticks\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    plt.colorbar(plt.cm.ScalarMappable(cmap=cmap, norm=norm), ax=ax)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    #return fig\n",
        "\n",
        "def argmax(a):\n",
        "    # random argmax\n",
        "    a = np.array(a)\n",
        "    return np.random.choice(np.arange(len(a), dtype=int)[a == np.max(a)])\n",
        "\n",
        "\n",
        "def Qlearn(build_env, alpha=0.2, gamma=0.99,epsilon = 1, min_epsilon=0.1, nsteps=800000, Qmat=None, callback_freq=5000, callback=None,):\n",
        "\n",
        "    episode_reward = 0\n",
        "    mean_episodic_reward = 0\n",
        "    n_episodes = 0\n",
        "    step_per_episode = []\n",
        "    step_count = 0\n",
        "    total_reward = 0\n",
        "    total_reward_acc = []\n",
        "    first_reward_flag = False\n",
        "    first_reward_index = 0\n",
        "    spaces_s = []\n",
        "\n",
        "\n",
        "    #env: gym.Env = build_env()\n",
        "    env = build_env\n",
        "    obs, info = env.reset()\n",
        "    if Qmat is None:\n",
        "      Qmat = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    step_check = int(nsteps/4)\n",
        "\n",
        "    plot_steps = [1,step_check,step_check*2,step_check*3,step_check*4-1 ]\n",
        "    pbar = tqdm(range(nsteps), colour=\"green\")\n",
        "    success_count = 0\n",
        "    episode_count = 0\n",
        "\n",
        "    all_cum_reward = []\n",
        "    #over_reward = []\n",
        "    for i in pbar:\n",
        "        step_count+=1\n",
        "        if np.random.rand() < epsilon:\n",
        "            # Exploration: choose a random action\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Exploitation: choose the action with the highest Q-value\n",
        "            action = argmax([Qmat[obs, i] for i in range(env.action_space.n)])\n",
        "\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action) #take an action\n",
        "        episode_reward += reward\n",
        "        total_reward+=reward\n",
        "        total_reward_acc.append(total_reward)\n",
        "        #over_reward.append(reward)\n",
        "\n",
        "        if not terminated and not truncated:# If we haven't reached target or failed\n",
        "            Q_next = max(Qmat[next_obs, action_next] for action_next in range(env.action_space.n))  # Get the maximum Q-value for the next state\n",
        "            # Update Q-value using Q-learning update rule\n",
        "            Qmat[obs, action] += alpha * (reward + gamma * Q_next - Qmat[obs, action])\n",
        "            obs = next_obs\n",
        "        else: #if we have reached the target or fell into a hole or reached the maximum amount of actions\n",
        "            # Episode ends, update mean episodic reward and reset environment\n",
        "            Qmat[obs, action] += alpha * (reward - Qmat[obs, action])\n",
        "            n_episodes += 1\n",
        "            mean_episodic_reward += (episode_reward - mean_episodic_reward) / n_episodes\n",
        "            episode_count+=1\n",
        "            episode_reward = 0\n",
        "            step_per_episode.append(step_count)\n",
        "            all_cum_reward.append((success_count/n_episodes)*100)\n",
        "            step_count = 0\n",
        "            if terminated and reward == 1:\n",
        "              success_count+=1\n",
        "              if first_reward_flag == False:\n",
        "                first_reward_flag = True\n",
        "                first_reward_index = i\n",
        "\n",
        "\n",
        "            obs, info = env.reset()\n",
        "\n",
        "        epsilon = max(min_epsilon, 0.999995 * epsilon)\n",
        "\n",
        "        if i % callback_freq == 0:\n",
        "            if callback is not None:\n",
        "                # Execute callback function if provided\n",
        "                callback(build_env, Qmat)\n",
        "        pbar.set_description(f\"Mean episodic reward: {mean_episodic_reward:.2f} | Epsilon: {epsilon:.2f} | Best reward: {best_reward:.2f}| Episode Count: {n_episodes}\")\n",
        "        if i in plot_steps:\n",
        "          #spaces_s.append([Qmat, \"Action space at step \"+str(i)])\n",
        "          plot_subdivided_heatmap(Qmat, title = \"Action space at step \"+str(i))\n",
        "\n",
        "    print(\"Nombre d'action effectuées avant la première récompense : \", first_reward_index)\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "    ax1.plot(step_per_episode)\n",
        "    if len(step_per_episode)>10:\n",
        "      ax1.plot(np.convolve(step_per_episode, np.ones(int(len(step_per_episode)/10))/int(len(step_per_episode)/10), mode='valid'), color = 'k', linestyle = '--')\n",
        "    ax1.set_title('Actions par épisodes')\n",
        "    ax2.plot(total_reward_acc)\n",
        "    ax2.set_title(\"Récompense totale\")\n",
        "    ax2.set_xlabel(\"Actions effectuées\")\n",
        "    ax2.axvline(first_reward_index, color = 'k', linestyle = '--')\n",
        "    plt.show()\n",
        "\n",
        "    #for i in spaces_s:\n",
        "      #print(i)\n",
        "      #plot_subdivided_heatmap(i[0], i[1])\n",
        "    return Qmat, Qbest\n",
        "\n",
        "def test(build_env, Qmat, test_steps=1000):\n",
        "    global Qbest, best_reward\n",
        "    #env: gym.Env = build_env()\n",
        "    env = build_env\n",
        "    n_episodes = 0\n",
        "    tot_rewards = 0\n",
        "    obs, info = env.reset()\n",
        "    for _ in range(test_steps):\n",
        "        # Choose the action with the highest Q-value\n",
        "        action = argmax([Qmat[obs, i] for i in range(env.action_space.n)])\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "        tot_rewards += reward\n",
        "        obs = next_obs\n",
        "        if terminated or truncated:\n",
        "            n_episodes += 1\n",
        "            obs, info = env.reset()\n",
        "    if best_reward < tot_rewards / n_episodes:\n",
        "        # Update the best reward and Q-best if a better reward is achieved\n",
        "        best_reward = tot_rewards / n_episodes\n",
        "        Qbest = deepcopy(Qmat)\n",
        "    return tot_rewards / n_episodes"
      ],
      "metadata": {
        "id": "xK-8ZnbQXRLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 1 :  Comprendre le modèle d'apprentissage chez un agent virtuel\n"
      ],
      "metadata": {
        "id": "CyegGWKQKdUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice 1 :  Exécutez les deux cellules suivantes et interprétez les résultats"
      ],
      "metadata": {
        "id": "NB1TdqeoKlXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Entrainer l'agent et comprendre les résultats\n",
        "n_steps = 5000\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.4\n",
        "Qbest = None\n",
        "best_reward = -np.inf\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "build_env = gym.make('FrozenLake-v1', desc=desc,  is_slippery=False)\n",
        "\n",
        "Qbest, QFinal = Qlearn(build_env, alpha=alpha, epsilon = epsilon, gamma=gamma, callback=test,callback_freq = int(n_steps/5), nsteps=n_steps)"
      ],
      "metadata": {
        "id": "2rCxaUvyXVtf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualiser le résultat de l'entrainement\n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode = \"rgb_array\")\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "obs, info = env.reset()\n",
        "n_episodes = 1\n",
        "\n",
        "map = QFinal\n",
        "for i in range(n_episodes):\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  obs, info = env.reset()\n",
        "  while not terminated and not truncated:\n",
        "      action = argmax([map[obs, i] for i in range(env.action_space.n)])\n",
        "      obs, reward, terminated, truncated, info = env.step(action)\n",
        "env.play()\n"
      ],
      "metadata": {
        "id": "Fp1hSA7zXc_X",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 2 : Comprendre les paramètres de notre agent"
      ],
      "metadata": {
        "id": "8y92nFyOPU0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 Le temps d'entrainement\n",
        "\n",
        "En vous basant sur vos manipulations du paramètre n_steps, quel est l'effet de manipuler le temps d'entrainement sur l'apprentissage?\n",
        "\n",
        "A quoi pourrait correspondre le temps d'entrainement dans un contexte d'apprentissage réel?"
      ],
      "metadata": {
        "id": "YiuvpyClQ0Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Effet du temps d'entrainement: Que se passe-t-il si vous réduisez progressivement le temps d'entrainement?\n",
        "n_steps = 300 # @param {type:\"slider\", min:100, max:5000, step:100}\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.4\n",
        "Qbest = None\n",
        "best_reward = -np.inf\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "build_env = gym.make('FrozenLake-v1', desc=desc,  is_slippery=False)\n",
        "\n",
        "Qbest, QFinal = Qlearn(build_env, alpha=alpha, epsilon = epsilon, gamma=gamma, callback=test,callback_freq = int(n_steps/5), nsteps=n_steps)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BejGcpyYPbFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualiser le résultat de l'entrainement\n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode = \"rgb_array\")\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "obs, info = env.reset()\n",
        "n_episodes = 1\n",
        "\n",
        "map = QFinal\n",
        "for i in range(n_episodes):\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  obs, info = env.reset()\n",
        "  while not terminated and not truncated:\n",
        "      action = argmax([map[obs, i] for i in range(env.action_space.n)])\n",
        "      obs, reward, terminated, truncated, info = env.step(action)\n",
        "env.play()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "By9cPFJKQDKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 :  Exploration et exploitation\n",
        "\n",
        "En vous basant sur vos manipulations du paramètre epsilon, quel est l'effet de manipuler le ratio exploration/exploitation?\n",
        "\n",
        "A quoi pourrait correspondre ce ratio dans un contexte d'apprentissage réel?"
      ],
      "metadata": {
        "id": "8FML6UktRE6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Effet du ratio exploitation/exploration: Que se passe-t-il si vous changez ce ratio?\n",
        "n_steps = 5000\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.4 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "Qbest = None\n",
        "best_reward = -np.inf\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "build_env = gym.make('FrozenLake-v1', desc=desc,  is_slippery=False)\n",
        "\n",
        "Qbest, QFinal = Qlearn(build_env, alpha=alpha, epsilon = epsilon, gamma=gamma, callback=test,callback_freq = int(n_steps/5), nsteps=n_steps)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1hLuUtASQLkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualiser le résultat de l'entrainement\n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode = \"rgb_array\")\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "obs, info = env.reset()\n",
        "n_episodes = 1\n",
        "\n",
        "map = QFinal\n",
        "for i in range(n_episodes):\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  obs, info = env.reset()\n",
        "  while not terminated and not truncated:\n",
        "      action = argmax([map[obs, i] for i in range(env.action_space.n)])\n",
        "      obs, reward, terminated, truncated, info = env.step(action)\n",
        "env.play()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0rpkVj55QYUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 :  Exploration et exploitation\n",
        "\n",
        "En vous basant sur vos manipulations du paramètre alpha, quel est l'effet de manipuler le taux d'apprentissage?\n",
        "\n",
        "A quoi pourrait correspondre ce taux dans un contexte d'apprentissage réel?"
      ],
      "metadata": {
        "id": "YeVjwP71RX1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Effet du taux d'apprentissage: Que se passe-t-il si vous changez ce taux?\n",
        "n_steps = 5000\n",
        "alpha = 0.1 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "gamma = 0.9\n",
        "epsilon = 0.4\n",
        "Qbest = None\n",
        "best_reward = -np.inf\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "build_env = gym.make('FrozenLake-v1', desc=desc,  is_slippery=False)\n",
        "\n",
        "Qbest, QFinal = Qlearn(build_env, alpha=alpha, epsilon = epsilon, gamma=gamma, callback=test,callback_freq = int(n_steps/5), nsteps=n_steps)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TvyM9LLyQbId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualiser le résultat de l'entrainement\n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode = \"rgb_array\")\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "obs, info = env.reset()\n",
        "n_episodes = 1\n",
        "\n",
        "map = QFinal\n",
        "for i in range(n_episodes):\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  obs, info = env.reset()\n",
        "  while not terminated and not truncated:\n",
        "      action = argmax([map[obs, i] for i in range(env.action_space.n)])\n",
        "      obs, reward, terminated, truncated, info = env.step(action)\n",
        "env.play()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VkaFsnjnQunR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 3 : Performance de l'agent"
      ],
      "metadata": {
        "id": "gRoXVN3_Rn0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 : Vous avez maintenant accès à tous les paramètres de l'agent, trouvez les paramètres qui selon vous maximisent les performances après apprentissage"
      ],
      "metadata": {
        "id": "vrBt274PRxXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Agent incluant tous les paramètres\n",
        "n_steps = 5000 # @param {type:\"slider\", min:100, max:5000, step:100}\n",
        "alpha = 0.1 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "gamma = 0.9\n",
        "epsilon = 0.9 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "Qbest = None\n",
        "best_reward = -np.inf\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "build_env = gym.make('FrozenLake-v1', desc=desc,  is_slippery=False)\n",
        "\n",
        "Qbest, QFinal = Qlearn(build_env, alpha=alpha, epsilon = epsilon, gamma=gamma, callback=test,callback_freq = int(n_steps/5), nsteps=n_steps)"
      ],
      "metadata": {
        "id": "EhdIzrKfSBoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualiser le résultat de l'entrainement\n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode = \"rgb_array\")\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "obs, info = env.reset()\n",
        "n_episodes = 1\n",
        "\n",
        "map = QFinal\n",
        "for i in range(n_episodes):\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  obs, info = env.reset()\n",
        "  while not terminated and not truncated:\n",
        "      action = argmax([map[obs, i] for i in range(env.action_space.n)])\n",
        "      obs, reward, terminated, truncated, info = env.step(action)\n",
        "env.play()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "znwipBN7SNXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5 : Nous allons maintenant placer notre agent entrainé dans un environnement modifié. Quelles sont les conséquences de cette modification?\n"
      ],
      "metadata": {
        "id": "Y4DS_Eo5-jUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Performance de l'agent dans un nouvel environnement\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFG\", \"HFFH\"]\n",
        "env = gym.make('FrozenLake-v1', desc=desc, map_name=\"4x4\", is_slippery=False, render_mode = \"rgb_array\")\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "obs, info = env.reset()\n",
        "n_episodes = 1\n",
        "\n",
        "map = QFinal\n",
        "for i in range(n_episodes):\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  obs, info = env.reset()\n",
        "  while not terminated and not truncated:\n",
        "      action = argmax([map[obs, i] for i in range(env.action_space.n)])\n",
        "      obs, reward, terminated, truncated, info = env.step(action)\n",
        "env.play()"
      ],
      "metadata": {
        "id": "Wt6BACi8L2PY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6 : Quelle solution pourrions nous proposez pour faire face aux modifications de l'environnement?"
      ],
      "metadata": {
        "id": "aDP7vLVKTXsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Votre réponse ici"
      ],
      "metadata": {
        "id": "mk2RCzYuTjBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7 : Nous allons maintenant ré-entrainer le même agent pendant un bref temps dans le nouvel environnement. Que pensez-vous de ses performances? Quelle solutions proposeriez vous pour les améliorer?"
      ],
      "metadata": {
        "id": "8DIhTLyoTmVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ré-entrainement du modèle"
      ],
      "metadata": {
        "id": "PBxv2r_b_JSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @Modèle complet\n",
        "n_steps = 1100 # @param {type:\"slider\", min:100, max:5000, step:100}\n",
        "\n",
        "Qbest = None\n",
        "best_reward = -np.inf\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFG\", \"HFFH\"]\n",
        "build_env = gym.make('FrozenLake-v1', desc=desc,  is_slippery=False)\n",
        "\n",
        "Qbest1, QFinal1 = Qlearn(build_env, alpha=alpha, epsilon = epsilon, gamma=gamma, callback=test,callback_freq = int(n_steps/5), nsteps=n_steps,Qmat = QFinal)\n"
      ],
      "metadata": {
        "id": "K7Id9jDghwLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Performance après ré-entrainement\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFG\", \"HFFH\"]\n",
        "env = gym.make('FrozenLake-v1', desc=desc, map_name=\"4x4\", is_slippery=False, render_mode = \"rgb_array\")\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "obs, info = env.reset()\n",
        "n_episodes = 1\n",
        "\n",
        "map = QFinal1\n",
        "for i in range(n_episodes):\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  obs, info = env.reset()\n",
        "  while not terminated and not truncated:\n",
        "      action = argmax([map[obs, i] for i in range(env.action_space.n)])\n",
        "      obs, reward, terminated, truncated, info = env.step(action)\n",
        "env.play()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dlAfenIEAlWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie 5 : Environnement complexe\n",
        "\n",
        "Nous allons maintenant placer notre agent dans un environnement plus complexe. En utilisant les mêmes paramètre exactement que pour le précédent agent, comparez l'apprentissage dans ce nouvel environnement."
      ],
      "metadata": {
        "id": "Xb38T-wbUwn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8 :  Qu'observez-vous? Comparez avec les résultats de l'agent ayant appris dans un environnement plus simple"
      ],
      "metadata": {
        "id": "remoj0c6Z0T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Environnement agrandi\n",
        "n_steps = 5000 # @param {type:\"slider\", min:100, max:10000, step:100}\n",
        "alpha = 0.1 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "gamma = 0.9\n",
        "epsilon = 0.9 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "Qbest = None\n",
        "best_reward = -np.inf\n",
        "desc=[\"SFFFFF\", \"FHFHFF\", \"HFFHFF\", \"HFHFFF\", \"HFHFHF\", \"FGHFHG\"]\n",
        "build_env = gym.make('FrozenLake-v1', desc=desc,  is_slippery=False)\n",
        "\n",
        "Qbest, QFinal = Qlearn(build_env, alpha=alpha, epsilon = epsilon, gamma=gamma, callback=test,callback_freq = int(n_steps/5), nsteps=n_steps)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "Bbx6gqR8Amk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualisation de l'agent dans un environnement agrandi\n",
        "env = gym.make('FrozenLake-v1', desc=desc, map_name=\"4x4\", is_slippery=False, render_mode = \"rgb_array\")\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "obs, info = env.reset()\n",
        "n_episodes = 1\n",
        "\n",
        "map = QFinal\n",
        "for i in range(n_episodes):\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  obs, info = env.reset()\n",
        "  while not terminated and not truncated:\n",
        "      action = argmax([map[obs, i] for i in range(env.action_space.n)])\n",
        "      obs, reward, terminated, truncated, info = env.step(action)\n",
        "env.play()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "f61WkMDqVG0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9 : En vous basant sur les résultats des agents évoluant dans les milieux simples et complexes, proposez une limites à la méthode d'apprentissage proposée ici"
      ],
      "metadata": {
        "id": "3hpSvu6TZq3T"
      }
    }
  ]
}